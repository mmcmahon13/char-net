{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 47552.0), ('to', 26268.0), ('and', 25179.0), ('of', 21444.0), ('a', 19852.0), ('.', 17618.0), ('was', 15155.0), ('he', 14435.0), ('his', 13612.0), ('said', 12682.0), ('in', 11952.0), ('Harry', 11326.0), ('had', 9954.0), ('that', 8840.0), ('at', 8322.0), ('you', 8044.0), ('it', 7310.0), ('I', 7282.0), ('as', 7026.0), ('with', 6272.0), ('on', 6097.0), ('for', 5193.0), ('He', 4990.0), ('not', 4666.0), ('\\xc2\\x91', 4527.0), ('her', 4418.0), ('they', 4259.0), ('be', 4258.0), ('were', 4155.0), ('but', 4010.0), ('from', 3997.0), ('have', 3970.0), ('out', 3847.0), ('him', 3806.0), ('\\xe2\\x80\\x94', 3561.0), ('she', 3462.0), ('up', 3432.0), ('into', 3427.0), ('Ron', 3366.0), ('all', 3336.0), ('The', 3250.0), ('been', 3136.0), ('Hermione', 3119.0), ('what', 2798.0), ('could', 2668.0), ('back', 2595.0), ('who', 2581.0), ('their', 2497.0), ('is', 2491.0), ('them', 2482.0), ('Harry,', 2400.0), ('\\xe2\\x80\\x9cI', 2326.0), ('looked', 2321.0), ('about', 2282.0), ('so', 2241.0), ('this', 2197.0), ('like', 2186.0), ('an', 2175.0), ('over', 2174.0), ('would', 2147.0), ('by', 2071.0), ('just', 2059.0), ('got', 1969.0), ('if', 1959.0), ('one', 1957.0), ('your', 1952.0), ('down', 1951.0), ('very', 1901.0), ('we', 1897.0), ('when', 1870.0), ('around', 1864.0), ('did', 1850.0), ('Dumbledore', 1727.0), ('Professor', 1723.0), ('do', 1687.0), ('my', 1666.0), ('are', 1655.0), ('me', 1646.0), ('more', 1634.0), ('Harry.', 1609.0), ('know', 1587.0), ('.\\xe2\\x80\\x9d', 1560.0), ('there', 1551.0), ('get', 1550.0), ('looking', 1550.0), ('think', 1548.0), ('though', 1544.0), ('off', 1524.0), ('see', 1523.0), ('still', 1521.0), ('through', 1487.0), ('no', 1473.0), ('It', 1466.0), ('\\xe2\\x80\\x94\\xe2\\x80\\x9d', 1456.0), ('or', 1452.0), ('CHAPTER', 1446.0), ('--', 1433.0), ('then', 1391.0), ('than', 1391.0), ('Ron,', 1384.0), ('which', 1377.0), ('him.', 1359.0), ('going', 1358.0), ('now', 1316.0), ('They', 1283.0), ('only', 1244.0), ('will', 1205.0), ('Mr.', 1191.0), ('him,', 1173.0), ('eyes', 1165.0), ('how', 1147.0), ('saw', 1144.0), ('said,', 1142.0), ('can', 1111.0), ('thought', 1110.0), ('before', 1109.0), ('And', 1101.0), ('seemed', 1100.0), ('THE', 1099.0), ('don\\xe2\\x80\\x99t', 1091.0), ('it,', 1089.0), ('She', 1070.0), ('where', 1070.0), ('told', 1053.0), ('look', 1051.0), ('behind', 1050.0), ('toward', 1042.0), ('time', 1041.0), ('Snape', 1034.0), ('A', 1033.0), ('But', 1025.0), ('never', 1016.0), ('\\xe2\\x80\\x9cYou', 1015.0), ('turned', 1009.0), ('wand', 991.0), ('go', 986.0), ('after', 985.0), ('Hagrid', 980.0), ('Harry\\xe2\\x80\\x99s', 977.0), ('it.', 946.0), ('Hermione,', 940.0), ('want', 935.0), ('come', 921.0), ('little', 915.0), ('something', 910.0), ('face', 909.0), ('made', 901.0), ('knew', 885.0), ('much', 883.0), ('asked', 877.0), ('even', 877.0), ('Weasley', 875.0), ('Mrs.', 867.0), ('other', 867.0), ('tell', 865.0), ('last', 864.0), ('heard', 864.0), ('felt', 863.0), ('There', 854.0), ('any', 842.0), ('some', 833.0), ('its', 832.0), ('door', 820.0), ('way', 794.0), ('himself', 791.0), ('two', 779.0), ('has', 775.0), ('long', 767.0), ('head', 767.0), ('might', 764.0), ('really', 757.0), ('right', 756.0), ('first', 754.0), ('front', 754.0), ('you,', 740.0), ('voice', 740.0), ('came', 740.0), ('because', 732.0), ('Ron.', 729.0), ('didn\\xe2\\x80\\x99t', 725.0), ('left', 724.0), ('Malfoy', 723.0), ('people', 706.0), ('hand', 703.0), ('took', 703.0), ('them.', 702.0), ('too', 700.0), ('make', 687.0), ('us', 682.0), ('trying', 680.0), ('again.', 674.0), ('few', 674.0), ('take', 673.0), ('away', 671.0), ('Voldemort', 660.0), ('it\\xe2\\x80\\x99s', 653.0), ('them,', 637.0), ('-', 635.0), ('say', 632.0), ('old', 631.0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "source_dir = \"C:\\\\Users\\\\Molly\\\\Google Drive\\\\senior classes\\\\nlp\\\\term_project\\\\book-nlp-master\\\\book-nlp-master\\\\data\\\\originalTexts\"\n",
    "# for filename in os.listdir(source_dir):\n",
    "#     print filename\n",
    "#     bookfile = open(source_dir + \"\\\\\" + filename, 'r')\n",
    "#     book = bookfile.read()\n",
    "#     bookDict = defaultdict(float)\n",
    "#     for word in book.split():\n",
    "#         bookDict[word] += 1\n",
    "#     counter = Counter(bookDict)\n",
    "#     top_n = counter.most_common(100)\n",
    "#     print top_n\n",
    "bookfile = open(source_dir + '\\\\full_hp.txt', 'r')\n",
    "book = bookfile.read()\n",
    "bookDict = defaultdict(float)\n",
    "for word in book.split():\n",
    "    bookDict[word] += 1\n",
    "counter = Counter(bookDict)\n",
    "top_n = counter.most_common(200)\n",
    "print top_n\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded json\n"
     ]
    }
   ],
   "source": [
    "wk_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "#json_filename = os.path.join(wk_dir, \"..\", \"booknlp_output\\\\potter.all.book.txt\")\n",
    "json_filename = \"C:\\\\Users\\\\Molly\\\\Google Drive\\\\senior classes\\\\nlp\\\\term_project\\\\char-net\\\\booknlp_output\\potter.all.book\";\n",
    "\n",
    "with open(json_filename) as json_file:\n",
    "\tchar_json = json.load(json_file)\n",
    "print \"successfully loaded json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', 'said', 'says', 'saying', 'whispered', 'went', 'seem', 'came', 'seemed', 'done', 'seen', 'made', 'told', 'come', 'use', 'used', 'cried', 'looked', 'knew', 'look', 'looked', 'shout', 'shouted', 'yelled', 'put', 'get', 'got', 'had', 'mutter', 'muttered', 'ask', 'asked', 'for', 'murmured', 'murmur', 'put', 'replied', 'reply', 'spoke', 'speaking', 'tell']\n"
     ]
    }
   ],
   "source": [
    "# TODO: add stopword removal (this is temporary, pull words from a file or something?)\n",
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "curDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "stop = os.path.join(curDir,'stopwords')     \n",
    "stopfile = open(stop, 'r')\n",
    "stop = stopfile.read()\n",
    "for word in stop.split():\n",
    "    stopwords.append(word)\n",
    "print stopwords\n",
    "print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dictionary of lists of character attributes, indexed by name (i.e. character's \"bag of words\")\n",
    "characters = {}\n",
    "\n",
    "for character in char_json[\"characters\"]:\n",
    "    # TODO: throw out characters that are mentioned less than 10 times (play with this threshold?)\n",
    "    if character[\"NNPcount\"] > 20:\n",
    "        char_name = character[\"names\"][0]['n']\n",
    "        characters[char_name] = []\n",
    "        for word_dict in character[\"patient\"]:\n",
    "            curWord = word_dict['w']\n",
    "            if curWord not in stopwords:\n",
    "                characters[char_name].append(curWord + \"_PATIENT\")\n",
    "        for word_dict in character[\"agent\"]:\n",
    "            curWord = word_dict['w']\n",
    "            if curWord not in stopwords:\n",
    "                characters[char_name].append(curWord + \"_AGENT\")\n",
    "        for word_dict in character[\"mod\"]:\n",
    "            curWord = word_dict['w']\n",
    "            if curWord not in stopwords:\n",
    "                characters[char_name].append(curWord + \"_MOD\")\n",
    "                \n",
    "        #TODO: add dialogue features?\n",
    "#         for word_dict in character['speaking']:\n",
    "#             print word_dict['w']\n",
    "                \n",
    "        # are things characters possess indicative of their character? Often not, so maybe exclude\n",
    "        for word_dict in character[\"poss\"]:\n",
    "            characters[char_name].append(word_dict['w'] + \"_POSS\")\n",
    "\n",
    "texts = [None for i in range(len(characters))]\n",
    "names = defaultdict(float)\n",
    "i = 0\n",
    "for key, value in characters.iteritems():\n",
    "\tprint str(key) + \": \" + str(value)\n",
    "\ttexts[i] = value\n",
    "\tnames[key] = i\n",
    "\ti += 1\n",
    "\tprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities \n",
    " \n",
    "# create Gensim dictionary from the texts\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# create bag of words model to perform analysis\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "numtopics = 4\n",
    "\n",
    "lda = models.LdaModel(corpus, num_topics=numtopics, \n",
    "                            id2word=dictionary, \n",
    "                            update_every=5, \n",
    "                            chunksize=100, \n",
    "                            passes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(lda.show_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# topics_matrix = lda.show_topics(num_topics=20, formatted=False, num_words=50)\n",
    "# topics_matrix = np.array(topics_matrix)\n",
    "\n",
    "# topic_words = topics_matrix[:,:,1]\n",
    "# for i in topic_words:\n",
    "#     print count\n",
    "#     print([str(word) for word in i])\n",
    "#     print()\n",
    "\n",
    "for i in range(0, numtopics):\n",
    "    print i\n",
    "    print lda.show_topic(i, 30)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_matrix = [[0 for i in range(numtopics)] for namekey in names]\n",
    "print len(topic_matrix)\n",
    "i = 0\n",
    "for namekey in names:\n",
    "    print str(namekey) + \" topic distribution: \"\n",
    "    for (key, value) in lda[corpus[names[namekey]]]:\n",
    "        topic_matrix[i][key] = value\n",
    "    i+=1\n",
    "    print lda[corpus[names[namekey]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array(topic_matrix)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 4\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(data)\n",
    "clusters = kmeans.labels_.tolist()\n",
    "print clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic={}\n",
    "i=0\n",
    "for nameKey in names:\n",
    "    dic[str(nameKey)]=clusters[i]\n",
    "    i += 1\n",
    "d=sorted(dic.items(), key=lambda x:x[1])\n",
    "for name in d:\n",
    "    print name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tempNameList = [nameKey for nameKey in names]\n",
    "charClusts = {'characters': tempNameList, 'clusters': clusters}\n",
    "frame = pd.DataFrame(charClusts, index = [clusters] , columns = ['character', 'clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame['clusters'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os  # for os.path.basename\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# get euclidean distance between each character's topic vector\n",
    "dist = euclidean_distances(data, data)\n",
    "\n",
    "MDS()\n",
    "\n",
    "# convert two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_colors = {0: 'b', 1: 'g', 2: 'r', 3: 'c'}\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: '0', \n",
    "                 1: '1',\n",
    "                 2: '2', \n",
    "                 3: '3', \n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The following is taken word for word from this tutorial: http://brandonrose.org/clustering\n",
    "\n",
    "#some ipython magic to show the matplotlib plots inline\n",
    "%matplotlib inline \n",
    "\n",
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=tempNameList)) \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
    "            label=cluster_names[name], color=cluster_colors[name], \n",
    "            mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelbottom='off')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelleft='off')\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=8)  \n",
    "\n",
    "    \n",
    "    \n",
    "plt.show() #show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#uncomment the below to save the plot if need be\n",
    "plt.savefig('clusters_small_noaxes.png', dpi=200)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 50)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=charClusts['characters']);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "\n",
    "#uncomment below to save figure\n",
    "plt.savefig('ward_clusters.png', dpi=200) #save figure as ward_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
